\section{Background}
\label{sec:background}

\subsection{Reservoir Computing}

Artificial Neural Networks (ANNs) are a commonly used computational model in machine learning and bio-inspired computing.
Simple, feed forward ANNs lend themselves well to problems were data can be spatially correlated, such as classification.
Many real world problems however, are temporal in nature.
Recurrent neural networks (RNNs) have been shown to be powerful tools for solving temporal problems such as stock market prediction~\cite{Lawrence2001}, learning context free/sensitive languages~\cite{Gers2001} and speech synthesis~\cite{Wu2016}.
Training RNNs is computationally expensive and often requires application specific adaptions of generalized training algorithms in order to reliably converge~\cite{Hammer2002}.
Several techniques have been proposed to circumvent problems related to training, such as Echo State Networks~\cite{Jaeger2001}, Liquid State Machines~\cite{Maass2002} and Backpropagation Decorrelation learning~\cite{Steil2004} \todo{Rephrase}.
These all share the common feature of only training weights of the output layer of the network.

Reservoir computing is an umbrella term for computational systems where a dynamic reservoir is excited by input data and output is generated by performing classification/regression over reservoir state.
Figure~\ref{fig:rc-system} shows the basic architecture of any reservoir computing system.

\begin{figure}
  \centering
  \begin{tikzpicture}[auto, node distance=2.5cm,>=latex']
    \node [block] (input) {Input};
    \node [block, right of=input] (reservoir) {Reservoir};
    \node [block, right of=reservoir] (readout) {Readout};
    \node [block, below of=reservoir] (f) {$f(x)$};
    \node [output, right of=readout] (output) {};

    \draw [->] (input) -- (reservoir);
    \draw [->] (reservoir) -- (readout);
    \draw [->] (readout) -- node [name=out] {Output} (output);
    \draw [->] (out) |- (f);
    \draw [->] (f) -- (reservoir);
  \end{tikzpicture}
  \caption{Basic overview of an RC system.}
\label{fig:rc-system}
\end{figure}

\subsection{Cellular Automata}

John von Neumann introduced cellular automata as a discrete computational model based on local interaction of cells on a grid of finite dimensionality.\todo{citation}
At any timestep $t$ during the simulation, each cell in the grid is in one of a finite number of states.
The state of any cell at time $t+1$ is computed as a function of the cells and its neighboring cells current states.

\subsection{Spiking Neural Networks}

Neural networks can be grouped into three generations, based on the characteristics of their base computational unit, the neuron.
The first generation, based on McCulloch-Pitts neurons, simple threshold gates, allows for universal computation on digital input/output values.
In the second generation, neurons apply a non-linear, continuous activation function on the weighted sum of their inputs.
Common activation functions include the sigmoid function, the hyperbolic tangent and, more recently, the rectifier.
