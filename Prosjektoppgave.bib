Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Fernando2003,
abstract = {This paper demonstrates that the waves produced on the surface of water can be used as the medium for a {\&}8220;Liquid State Machine{\&}8221; that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass{\&}8217; Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this {\&}8220;for free{\&}8221;, and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.},
author = {Fernando, Chrisantha and Sojakka, Sampsa},
doi = {10.1007/978-3-540-39432-7_63},
isbn = {9783540200574},
issn = {03029743},
journal = {Advances in Artificial Life},
pages = {588--597},
title = {{Pattern Recognition in a Bucket}},
url = {http://www.springerlink.com/content/xlnymhf0qp946rce},
year = {2003}
}
@article{Lawrence2001,
abstract = {Financial forecasting is an example of a signal processing problem which is challenging due to small sample sizes, high noise, non-stationarily, and non-linearity. Neural networks have been very successful in a number of signal processing applications. We discuss fundamental limitations and inherent difficulties when using neural networks for the processing of high noise, small sample size signals. We introduce a new intelligent signal processing method which addresses the difficulties. The method proposed uses conversion into a symbolic representation with a self-organizing map, and grammatical inference with recurrent neural networks. We apply the method to the prediction of daily foreign exchange rates, addressing difficulties with non-stationarily, overfitting, and unequal a priori class probabilities, and we find significant predictability in comprehensive experiments covering 5 different foreign exchange rates. The method correctly predicts the direction of change for the next day with an error rate of 47.1{\%}. The error rate reduces to around 40{\%} when rejecting examples where the system has low confidence in its prediction. We show that the symbolic representation aids the extraction of symbolic knowledge from the trained recurrent neural networks in the form of deterministic finite state automata. These automata explain the operation of the system and are often relatively simple. Automata rules related to well known behavior such as trend following and mean reversal are extracted.},
author = {Lawrence, Steve and Giles, CL Lee and Fong, S and Lawrence, Steve and Tsoi, Ah Chung AC},
doi = {10.1023/A:1010884214864},
issn = {0885-6125},
journal = {Machine Learning},
number = {1},
pages = {161--183},
title = {{Noisy Time Series Prediction using Recurrent Neural Networks and Grammatical Inference}},
url = {http://link.springer.com/article/10.1023/A:1010884214864$\backslash$nhttp://www.springerlink.com/index/X15G111873157W14.pdf$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=842255},
volume = {44},
year = {2001}
}
@article{Yilmaz2014,
abstract = {We introduce a novel framework of reservoir computing. Cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is capable of long short-term memory and it requires orders of magnitude less computation compared to Echo State Networks. Also, for additive cellular automaton rules, reservoir features can be combined using Boolean operations, which provides a direct way for concept building and symbolic processing, and it is much more efficient compared to state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.0162v1},
author = {Yilmaz, Ozgur},
eprint = {arXiv:1410.0162v1},
journal = {arXiv preprint},
pages = {1--9},
title = {{Reservoir Computing using Cellular Automata}},
year = {2014}
}
@article{Hammer2002,
abstract = {... restrict recurrent connections to self-connections of the units such that backpropagation formulas can be ... Noisy time series prediction using a RNN and grammatical inference. ... RNNs: Design and Applications, chapter Comparison of Recurrent  Networks for Trajectory Generation. ...},
author = {Hammer, Barbara and Steil, Jochen J.},
isbn = {2930307021},
journal = {Proc. ESANN},
number = {April},
pages = {357--368},
title = {{Tutorial: Perspectives on learning with rnns}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5565{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@article{Maass2002,
abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Maass, Wolfgang and Natschl{\"{a}}ger, Thomas and Markram, Henry},
doi = {10.1162/089976602760407955},
eprint = {arXiv:1011.1669v3},
isbn = {0899-7667 (Print)$\backslash$n0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {11},
pages = {2531--2560},
pmid = {12433288},
title = {{Real-time computing without stable states: a new framework for neural computation based on perturbations.}},
volume = {14},
year = {2002}
}
@inproceedings{Wu2016,
abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
archivePrefix = {arXiv},
arxivId = {1601.02539},
author = {Wu, Zhizheng and King, Simon},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2016.7472657},
eprint = {1601.02539},
isbn = {9781479999880},
issn = {15206149},
keywords = {Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network},
pages = {5140--5144},
title = {{Investigating gated recurrent networks for speech synthesis}},
volume = {2016-May},
year = {2016}
}
@article{Gers2001,
abstract = {Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b(n)c(n).},
author = {Gers, F. A. and Schmidhuber, J.},
doi = {10.1109/72.963769},
isbn = {1045-9227 VO  - 12},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Context-free languages (CFLs),Context-sensitive languages (CSLs),Long short-term memory (LSTM),Recurrent neural networks (RNNs)},
number = {6},
pages = {1333--1340},
pmid = {18249962},
title = {{LSTM recurrent networks learn simple context-free and context-sensitive languages}},
volume = {12},
year = {2001}
}
@article{Verstraeten2007,
abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Verstraeten, D. and Schrauwen, B. and D'Haene, M. and Stroobandt, D.},
journal = {Neural Networks},
keywords = {Chaos,Lyapunov exponent,Memory capability,Reservoir computing},
month = {apr},
number = {3},
pages = {391--403},
title = {{An experimental unification of reservoir computing methods}},
volume = {20},
year = {2007}
}
@article{Maass1997,
abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
author = {Maass, Wolfgang},
doi = {10.1016/S0893-6080(97)00011-7},
isbn = {08936080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
number = {9},
pages = {1659--1671},
title = {{Networks of spiking neurons: The third generation of neural network models}},
volume = {10},
year = {1997}
}
@article{Snyder2013,
abstract = {This paper underscores the conjecture that intrinsic computation is maximal in systems at the “edge of chaos”. We study the relationship between dynamics and computational capability in random Boolean networks (RBN) for reservoir computing (RC). RC is a computational paradigm in which a trained readout layer interprets the dynamics of an excitable component (called the reservoir) that is perturbed by external input. The reservoir is often implemented as a homogeneous recurrent neural network, but there has been little investigation into the properties of reservoirs that are discrete and heterogeneous. Random Boolean networks are generic and heterogeneous dynamical systems and here we use them as the reservoir. A RBN is typically a closed system; to use it as a reservoir we extend it with an input layer. As a consequence of perturbation, the RBN does not necessarily fall into an attractor. Computational capability in RC arises from a tradeoff between separability and fading memory of inputs. We find the balance of these properties predictive of classification power and optimal at critical connectivity. These results are relevant to the construction of devices which exploit the intrinsic dynamics of complex heterogeneous systems, such as biomolecular substrates.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.1744v1},
author = {Snyder, David and Goudarzi, Alireza and Teuscher, Christof},
doi = {10.1103/PhysRevE.87.042808},
eprint = {arXiv:1212.1744v1},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {4},
title = {{Computational capabilities of random automata networks for reservoir computing}},
volume = {87},
year = {2013}
}
@article{Jaeger2001,
abstract = {The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task.},
author = {Jaeger, Herbert},
doi = {citeulike-article-id:9635932},
isbn = {0-7803-9048-2},
issn = {18735223},
journal = {GMD Report},
pages = {1--47},
pmid = {19036266},
title = {{The "echo state" approach to analysing and training recurrent neural networks}},
volume = {148},
year = {2001}
}
@inproceedings{Steil2004,
abstract = {We introduce a new learning rule for fully recurrent neural networks which we call backpropagation-decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.08836v1},
author = {Steil, Jochen J.},
booktitle = {IEEE International Conference on Neural Networks - Conference Proceedings},
doi = {10.1109/IJCNN.2004.1380039},
eprint = {arXiv:1506.08836v1},
isbn = {0780383591},
issn = {10987576},
pages = {843--848},
title = {{Backpropagation-Decorrelation: Online recurrent learning with O(N) complexity}},
volume = {2},
year = {2004}
}
